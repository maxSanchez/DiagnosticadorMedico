{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimento 4\n",
    "Después de intentar usar gensim para generar un modelo usando redes neuronales, opté por realizar una propuesta más intuitiva, que obedece el siguiente diagrama:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](diagrama.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por pasos tenemos el siguiente código, primero necesitamos una función que lea el corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leer(corpus):\n",
    "\twith open(corpus,\"r\") as f:\n",
    "\t\tfor line in f:\n",
    "\t\t\tyield line.split(\"|\",1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Después dado que TfidfVectorizer regresa una matriz con los valores de tf-idf, necesitamos extraer el vocabulario y el valor tf-idf asociado, con el siguiente código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(coo_matrix,feature_names):\n",
    "\tpuntos=[]\n",
    "\tnombres=[]\n",
    "\tfor score,idx in zip(coo_matrix.data,coo_matrix.col):\n",
    "\t\taux=feature_names[idx].rstrip('sao') \n",
    "\t\tif aux not in nombres:\n",
    "\t\t\tpuntos.append(score)\n",
    "\t\t\tnombres.append(aux)\n",
    "\t\telif score>puntos[nombres.index(aux)]: #aqui podemos ver que tomo el tf más grande de todos\n",
    "            #principal diferencia con el experimento 5\n",
    "\t\t\tpuntos[nombres.index(aux)]=score\n",
    "\ttup=zip(nombres,puntos)\n",
    "\treturn sorted(tup,key=lambda x: x[1],reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que tenemos los valores tf-idf, es necesario calcular los vectores de cada documento, con el siguiente código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import numpy as np\n",
    "            \n",
    "def doc2Vec(doc,vocab,funcion):\n",
    "\tpalabras=list(map(lambda x: unicodedata.normalize('NFKD',x).rstrip('sao'),doc.split()))\n",
    "\tvector=[]\n",
    "\tfor key,value in vocab.items():\n",
    "\t\tvector.append(funcion(palabras.count(key))*value)\n",
    "\treturn np.array(vector).reshape(1,-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego debemos de tener una función que calcule los vectores de todos los documentos e integre las funciones anteriores, ésta es la siguiente: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/max/.local/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:286: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['afirmo', 'agrego', 'algun', 'anadio', 'aseguro', 'comento', 'considero', 'dejo', 'demas', 'estabamos', 'estara', 'estaran', 'estaras', 'estare', 'estareis', 'estaria', 'estariais', 'estariamos', 'estarian', 'estarias', 'esteis', 'esten', 'estes', 'estuvieramos', 'estuviesemos', 'explico', 'expreso', 'fueramos', 'fuesemos', 'habeis', 'habiais', 'habiamos', 'habian', 'habias', 'habra', 'habran', 'habras', 'habre', 'habreis', 'habria', 'habriais', 'habriamos', 'habrian', 'habrias', 'hayais', 'hubieramos', 'hubiesemos', 'indico', 'llego', 'manifesto', 'menciono', 'ningun', 'podra', 'podran', 'proximos', 'quedo', 'realizo', 'seais', 'senalo', 'seran', 'seras', 'sere', 'sereis', 'seria', 'seriais', 'seriamos', 'serian', 'serias', 'tendra', 'tendran', 'tendras', 'tendre', 'tendreis', 'tendria', 'tendriais', 'tendriamos', 'tendrian', 'tendrias', 'tengais', 'tenia', 'teniais', 'teniamos', 'tenian', 'tenias', 'traves', 'tuvieramos', 'tuviesemos', 'ultima', 'ultimas', 'ultimos'] not in stop_words.\n",
      "  sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Me tarde :  48.80319881439209\n",
      "4538\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pickle\n",
    "import time\n",
    "from math import exp\n",
    "\n",
    "def crearVectores(archCorpus,archPickle,funcion):\n",
    "    # se usa un set de palabras que deben detenerse, lo encontré en este repo.\n",
    "    # https://github.com/stopwords-iso/stopwords-es\n",
    "\twith open(\"../Experimento3/stopwords-es-master/stopwords-es.txt\",\"r\") as f:\n",
    "\t    stopwords=f.read().splitlines() \n",
    "\tcorpus=list(leer(archCorpus)) # leemos el corpus\n",
    "\tvectorizador=TfidfVectorizer(smooth_idf=True, #aplicamos un suavizado del idf, no afecta mucho al resultado\n",
    "                                 use_idf=True, # usamos idf\n",
    "                                 stop_words=stopwords, # le damos el set de palabras a deterner\n",
    "                                 min_df=2, # queremos que las palabras aparezcan al menos en dos documentos\n",
    "                                 #si cambiamos esto a 1, pone muchas palabras que solo hacen el programa mas lento\n",
    "                                 strip_accents='ascii') # le aplica una normalización a las palabras\n",
    "\tpalabras=vectorizador.fit_transform(map(lambda x:x[1],corpus)) #aqui se aplica el tf-idf\n",
    "\tfeature_names=vectorizador.get_feature_names() #se obtine el vocab\n",
    "\tkeywords=dict(getData(palabras.tocoo(),feature_names)) # obtenemos el vocab relacionado con sus tf-idf\n",
    "\tvectores={} \n",
    "\tcont=0\n",
    "\ttotal=len(corpus)\n",
    "\tti=time.time()\n",
    "\tfor c in corpus: #para cada documentos c en el corpus\n",
    "\t\tvectores[c[0]]=doc2Vec(c[1],keywords,funcion) # se transforma a un vector\n",
    "\t\tprint(\"llevo \",cont,\" de \",total,end=\"\\r\") # contamos cuantos lleva\n",
    "\t\tcont+=1\n",
    "\tprint(\"Me tarde : \",time.time()-ti)\n",
    "\tpickle.dump([vectores,keywords],open(archPickle,\"wb\")) #lo guardamos en un pickle para despues\n",
    "\treturn [vectores,keywords] #regresamos los vectores y el vocab\n",
    "funcion=lambda x:1/(1+exp(5*(1-x))) #Función sigmoide para tener los valores en un rango 0 y 1\n",
    "\n",
    "vectores,vocab=crearVectores(\"datosLimpios.csv\",\"vectoresExp4.pkl\",funcion)\n",
    "print(len(vocab))\n",
    "#esto suele tardar 48 segundos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que tenemos los vectores de los documentos, es necesario comparar cada uno con el vector que describe nuestros síntomas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "def predecir(archSintomas,vectores,vocab,funcion):\n",
    "\tsintomas=open(archSintomas,\"r\").read()\n",
    "\tvector=doc2Vec(sintomas,vocab,funcion)\n",
    "\tdocs=[]\n",
    "\tfor n,v in vectores.items():\n",
    "\t\t# docs.append([n,distance.cdist( vector,v, 'wminkowski',w=np.random.rand(v.shape[0]))])\n",
    "\t\t# docs.append([n,distance.cdist( vector,v, 'matching')])\n",
    "\t\t# docs.append([n,distance.cdist( vector,v, 'braycurtis')])\n",
    "\t\t# docs.append([n,distance.cdist( vector,v, 'canberra')])\n",
    "\t\t# docs.append([n,distance.cdist( vector,v, 'chebyshev')])\n",
    "\t\t# docs.append([n,distance.cdist( vector,v, 'jaccard')])\n",
    "\t\t# docs.append([n,distance.cdist( vector,v, 'correlation')])\n",
    "\t\t# docs.append([n,distance.cdist( vector,v, 'sqeuclidean')])\n",
    "\t\t# docs.append([n,distance.cdist( vector,v, 'cityblock')])\n",
    "\t\tdocs.append([n,distance.cdist( vector,v, 'cosine')]) #es la que mejor sirve\n",
    "\t\t# docs.append([n,distance.cdist( vector,v, 'euclidean')])\n",
    "\tdocs.sort(key=lambda x:x[1])\n",
    "\treturn docs\n",
    "dists=predecir(\"misSintomas.txt\",vectores,vocab,funcion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya por último, tenemos que mostrar el nombre de las enfermedades más cercanas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['espondilitis_anquilosante.html ', array([[0.55964787]])]\n",
      "['sacralgia.html ', array([[0.57724832]])]\n",
      "['dolor_espalda.html ', array([[0.58849898]])]\n",
      "['hipermetropia.html ', array([[0.63843594]])]\n",
      "['ciatica.html ', array([[0.67218111]])]\n",
      "['enfermedad_sudeck.html ', array([[0.68650639]])]\n",
      "['hiperparatiroidismo.html ', array([[0.68776322]])]\n",
      "['cefalea_en_racimos.html ', array([[0.69026291]])]\n",
      "['dismenorrea.html ', array([[0.70119622]])]\n",
      "['epididimitis.html ', array([[0.71563155]])]\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "\tprint(dists[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusiones\n",
    "Como conclusión este sistema funciona relativamente bien, ya que no siempre regresa las enfermedades que son más intuitivas para un médico, este sistema no sabe que hay enfermedades comunes y otras que no, también realicé otro experimento donde encuentro los tf-idf por sin usar tdfvectorizer, ver Experimento5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cancer-cabeza-cuello.html ', array([[0.78392699]])]\n",
      "['ronquido.html ', array([[0.79196845]])]\n",
      "['piedras_en_la_vesicula.html ', array([[0.80235308]])]\n",
      "['leucemia.html ', array([[0.80323531]])]\n",
      "['apnea_del_sueno.html ', array([[0.80835584]])]\n",
      "['bruxismo.html ', array([[0.80857495]])]\n",
      "['leucoplasia.html ', array([[0.81379813]])]\n",
      "['arteriosclerosis.html ', array([[0.81670861]])]\n",
      "['enfermedad_mano_pie_boca.html ', array([[0.82025684]])]\n",
      "['quiste_ovario.html ', array([[0.82574736]])]\n"
     ]
    }
   ],
   "source": [
    "dists=predecir(\"misSintomas2.txt\",vectores,vocab,funcion) #sintomas alergia\n",
    "for i in range(10):\n",
    "    print(dists[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hirsutismo.html ', array([[0.73576919]])]\n",
      "['tenosinovitis.html ', array([[0.76819148]])]\n",
      "['sudor_de_manos.html ', array([[0.80680944]])]\n",
      "['cirrosis_hepatica.html ', array([[0.81389452]])]\n",
      "['diarrea.html ', array([[0.81656632]])]\n",
      "['hiperhidrosis.html ', array([[0.81729501]])]\n",
      "['polipos_intestinales.html ', array([[0.81901033]])]\n",
      "['narcolepsia.html ', array([[0.82030984]])]\n",
      "['apnea_del_sueno.html ', array([[0.82505592]])]\n",
      "['sordera_subita.html ', array([[0.82709366]])]\n"
     ]
    }
   ],
   "source": [
    "dists=predecir(\"misSintomas3.txt\",vectores,vocab,funcion) #sintomas gripe \n",
    "for i in range(10):\n",
    "    print(dists[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gripe_porcina.html ', array([[0.47269598]])]\n",
      "['resfriado.html ', array([[0.50981528]])]\n",
      "['legionela.html ', array([[0.58280845]])]\n",
      "['malaria.html ', array([[0.5900416]])]\n",
      "['gripe.html ', array([[0.63739934]])]\n",
      "['bronquiectasia.html ', array([[0.68786715]])]\n",
      "['miocarditis.html ', array([[0.69214178]])]\n",
      "['sarampion.html ', array([[0.7146384]])]\n",
      "['bronquiolitis.html ', array([[0.72048584]])]\n",
      "['escarlatina.html ', array([[0.72448795]])]\n"
     ]
    }
   ],
   "source": [
    "dists=predecir(\"prueba.txt\",vectores,vocab,funcion) #sintomas gripe de otra pagina\n",
    "for i in range(10):\n",
    "    print(dists[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como conclusión podemos decir que el sistema funciona relativamente bien, ya que muestra enfermedades con sintomas parecidos, aunque no prioriza la enfermedades mas probables, y con una buena descripción de los sintomas funciona bien."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
